{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bab179ae",
   "metadata": {},
   "source": [
    "# Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da45fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# обучение\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# utils\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# логирование и версионирование экспериментов\n",
    "from clearml import Task, TaskTypes, OutputModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b780492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверяем доступность видеодрайвера\n",
    "\n",
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "print(\"CUDA version: \", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d8e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# фиксируем гиперпараметры предстоящего эксперимента\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "data_augment = '2'\n",
    "num_epochs = 6\n",
    "\n",
    "# будет также использоваться позднее при сохранении модели и чекпоинтов, обязательно менять перед началом нового эксперимента!\n",
    "experiment_name = f'EffNetB7_batch_{batch_size}_lr_{learning_rate}_augment_{data_augment}_epochs_{num_epochs}' \n",
    "\n",
    "# создаем папку, куда будем сохранять чекпоинты, веса и метрики\n",
    "exp_dir_path = f\"experiments/{experiment_name}\"\n",
    "os.makedirs(exp_dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839c64a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# устанавливаем соединение с сервером ClearML\n",
    "\n",
    "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "\n",
    "# mfdp\n",
    "%env CLEARML_API_ACCESS_KEY=%insert_access_key%\n",
    "%env CLEARML_API_SECRET_KEY=%insert_secret_key%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eeee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализируем новый ClearML task\n",
    "\n",
    "task = Task.init(\n",
    "        project_name='MyFirstDataProject',\n",
    "        task_name=experiment_name,\n",
    "        tags= [f'data_augmentation_{data_augment}', f'batch_size_{batch_size}', \n",
    "               f'learning_rate_{learning_rate}', f'num_epochs_{num_epochs}'],\n",
    "        task_type=TaskTypes.training,\n",
    "        auto_connect_frameworks={'pytorch':True},\n",
    "        auto_connect_streams={'stdout': True, 'stderr': True}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.files = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            self.files += [(os.path.join(class_dir, f), class_name) for f in os.listdir(class_dir)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_name = self.files[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.classes.index(class_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9325c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize to [-1, 1] for each channel\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e6d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageDataset('dataset/train_dataset_transformed_2', transform=transform)\n",
    "test_data = ImageDataset('dataset/test_dataset/', transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(test_data, batch_size=128, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87b7f11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "num_ftrs = model._fc.in_features\n",
    "model._fc = nn.Linear(num_ftrs, len(train_data.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7113a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a38a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"loading model to device\")\n",
    "model = model.to(device)\n",
    "print(\"model loaded\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'Using {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "    \n",
    "# переводим модель в training mode\n",
    "model.train()  \n",
    "\n",
    "# устанавливаем seed для воспроизводимости\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    \n",
    "    \n",
    "# определяем критерии остановки обучения если необходимо\n",
    "patience = 5   # количество эпох без улучшения до остановки обучения \n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(10,15):\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
    "    \n",
    "    for i, (images, labels) in progress_bar:  \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "\n",
    "        outputs = model(images)  # forward pass\n",
    "        loss = criterion(outputs, labels)  # compute the loss\n",
    "        loss.backward()  # backward pass\n",
    "        optimizer.step()  # update the weights\n",
    "\n",
    "        # сохраняем лейблы и предикшены для расчета метрик\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} Loss: {running_loss/(i+1):.2f}\")\n",
    "\n",
    "    # рассчитываем метрики\n",
    "    precision = precision_score(all_labels, all_predictions, average=None)\n",
    "    recall = recall_score(all_labels, all_predictions, average=None)\n",
    "    f1 = f1_score(all_labels, all_predictions, average=None)\n",
    "    avg_precision = precision_score(all_labels, all_predictions, average='macro')\n",
    "    avg_recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "    avg_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_data)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {avg_precision:.4f}, \\\n",
    "Recall: {avg_recall:.4f}, F1 Score: {avg_f1:.4f}\")\n",
    "    \n",
    "    scheduler.step(epoch_loss)  # step the learning rate scheduler\n",
    "    \n",
    "\n",
    "    # проверка предварительной остановки обучения\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_epoch = epoch+1\n",
    "        model_saving_name = f\"{exp_dir_path}/{experiment_name}_best.pth\"\n",
    "        \n",
    "        # сохраняем лучшую модель\n",
    "        torch.save(model.state_dict(), model_saving_name)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print('Оставливаем обучение, модель перестала обучаться')\n",
    "            break\n",
    "    \n",
    "    \n",
    "    # сохраняем последний чекпоинт на случай прерывания обучения\n",
    "    torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict' : scheduler.state_dict(),\n",
    "    'loss': loss,\n",
    "    }, f\"{exp_dir_path}/{experiment_name}_checkpoint.pth\")\n",
    "    \n",
    "    \n",
    "    # логируем метрики в ClearML\n",
    "    logger = task.get_logger()\n",
    "    logger.report_scalar('Loss', 'train', epoch_loss, epoch)\n",
    "    logger.report_scalar('Accuracy', 'train', accuracy, epoch)\n",
    "    logger.report_scalar('Average Metrics/Precision', 'train', avg_precision, epoch)\n",
    "    logger.report_scalar('Average Metrics/Recall', 'train', avg_recall, epoch)\n",
    "    logger.report_scalar('Average Metrics/F1', 'train', avg_f1, epoch)\n",
    "    for class_id, (p, r, f) in enumerate(zip(precision, recall, f1)):\n",
    "        class_name = train_data.classes[class_id] \n",
    "        logger.report_scalar('Metrics by Class/Precision', class_name, p, epoch)\n",
    "        logger.report_scalar('Metrics by Class/Recall', class_name, r, epoch)\n",
    "        logger.report_scalar('Metrics by Class/F1', class_name, f, epoch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Переключение модели в режим оценки\n",
    "    model.eval()\n",
    "\n",
    "    all_test_labels = []\n",
    "    all_test_predictions = []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    # Итерация по тестовому набору данных\n",
    "    progress_bar = tqdm(enumerate(test_loader), total=len(test_loader), leave=False)\n",
    "    for i, (images, labels) in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Предсказания модели\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "\n",
    "        # Вычисление потерь\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Сохранение меток и предсказаний\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        all_test_labels.extend(labels.cpu().numpy())\n",
    "        all_test_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Вычисление финальных метрик\n",
    "    test_loss = running_loss / len(test_data)\n",
    "    test_precision = precision_score(all_test_labels, all_test_predictions, average='macro')\n",
    "    test_recall = recall_score(all_test_labels, all_test_predictions, average='macro')\n",
    "    test_f1 = f1_score(all_test_labels, all_test_predictions, average='macro')\n",
    "    test_accuracy = accuracy_score(all_test_labels, all_test_predictions)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test Precision: {test_precision:.4f}, \\\n",
    "Test Recall: {test_recall:.4f}, Test F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    # Логирование метрик в ClearML\n",
    "    logger = task.get_logger()\n",
    "    logger.report_scalar('Loss', 'test', test_loss, epoch)\n",
    "    logger.report_scalar('Accuracy', 'test', test_accuracy, epoch)\n",
    "    logger.report_scalar('Average Metrics/Precision', 'test', test_precision, epoch)\n",
    "    logger.report_scalar('Average Metrics/Recall', 'test', test_recall, epoch)\n",
    "    logger.report_scalar('Average Metrics/F1', 'test', test_f1, epoch)\n",
    "\n",
    "    # Логирование метрик в ClearML по классам\n",
    "    test_precision_class = precision_score(all_test_labels, all_test_predictions, average=None)\n",
    "    test_recall_class = recall_score(all_test_labels, all_test_predictions, average=None)\n",
    "    test_f1_class = f1_score(all_test_labels, all_test_predictions, average=None)\n",
    "\n",
    "    for class_id, (p, r, f) in enumerate(zip(test_precision_class, test_recall_class, test_f1_class)):\n",
    "        class_name = test_data.classes[class_id]  # get class name from class id\n",
    "        logger.report_scalar('Test Metrics by Class/Precision', class_name, p, epoch)\n",
    "        logger.report_scalar('Test Metrics by Class/Recall', class_name, r, epoch)\n",
    "        logger.report_scalar('Test Metrics by Class/F1', class_name, f, epoch)\n",
    "    \n",
    "    model.train() # переводим модель обратно в training mode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30ca0e15",
   "metadata": {},
   "source": [
    "## Оценка результатов обучения на тестовой выборке, логирование в ClearML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd622a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Загрузка лучшей сохраненной модели\n",
    "model_path = model_saving_name\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Переключение модели в режим оценки\n",
    "model.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "running_loss = 0.0\n",
    "\n",
    "# Итерация по тестовому набору данных\n",
    "progress_bar = tqdm(enumerate(test_loader), total=len(test_loader), leave=False)\n",
    "for i, (images, labels) in progress_bar:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Предсказания модели\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "\n",
    "    # Вычисление потерь\n",
    "    loss = criterion(outputs, labels)\n",
    "    running_loss += loss.item() * images.size(0)\n",
    "\n",
    "    # Сохранение меток и предсказаний\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "    all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Вычисление финальных метрик\n",
    "test_loss = running_loss / len(test_data)\n",
    "test_precision = precision_score(all_labels, all_predictions, average='macro')\n",
    "test_recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "test_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "test_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.2f}, Test Accuracy: {test_accuracy:.2f}, Test Precision: {test_precision}, \\\n",
    "Test Recall: {test_recall}, Test F1 Score: {test_f1}\")\n",
    "\n",
    "# Логирование метрик в ClearML\n",
    "logger = task.get_logger()\n",
    "logger.report_scalar('Loss', 'test', test_loss, best_epoch-1)\n",
    "logger.report_scalar('Accuracy', 'test', test_accuracy, best_epoch-1)\n",
    "logger.report_scalar('Average Metrics/Precision', 'test', test_precision, best_epoch-1)\n",
    "logger.report_scalar('Average Metrics/Recall', 'test', test_recall, best_epoch-1)\n",
    "logger.report_scalar('Average Metrics/F1', 'test', test_f1, best_epoch-1)\n",
    "\n",
    "# Логирование метрик в ClearML по классам\n",
    "test_precision_class = precision_score(all_labels, all_predictions, average=None)\n",
    "test_recall_class = recall_score(all_labels, all_predictions, average=None)\n",
    "test_f1_class = f1_score(all_labels, all_predictions, average=None)\n",
    "\n",
    "for class_id, (p, r, f) in enumerate(zip(test_precision_class, test_recall_class, test_f1_class)):\n",
    "    class_name = test_data.classes[class_id]  # get class name from class id\n",
    "    logger.report_scalar('Test Metrics by Class/Precision', class_name, p, best_epoch-1)\n",
    "    logger.report_scalar('Test Metrics by Class/Recall', class_name, r, best_epoch-1)\n",
    "    logger.report_scalar('Test Metrics by Class/F1', class_name, f, best_epoch-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57f4e165",
   "metadata": {},
   "source": [
    "# Сохранение результатов, построение confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8256fbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5ddcd4f",
   "metadata": {},
   "source": [
    "## Сохранение метрик в csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e35920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим словарь для всех метрик\n",
    "results_dict = {\n",
    "    'test_loss': [test_loss],\n",
    "    'test_accuracy': [test_accuracy],\n",
    "    'test_precision_macro': [test_precision],\n",
    "    'test_recall_macro': [test_recall],\n",
    "    'test_f1_macro': [test_f1],\n",
    "}\n",
    "\n",
    "# Добавим в словарь метрики по классам\n",
    "for class_id, (p, r, f) in enumerate(zip(test_precision_class, test_recall_class, test_f1_class)):\n",
    "    class_name = test_data.classes[class_id]  # get class name from class id\n",
    "    results_dict[f'{class_name}_precision'] = [p]\n",
    "    results_dict[f'{class_name}_recall'] = [r]\n",
    "    results_dict[f'{class_name}_f1'] = [f]\n",
    "\n",
    "# Преобразуем словарь в DataFrame\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Сохраняем результаты в CSV-файл\n",
    "results_df.to_csv(f'{exp_dir_path}/{experiment_name}.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cbe2f9a",
   "metadata": {},
   "source": [
    "## Вывод матрицы ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ddb704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(all_test_labels, all_test_predictions)\n",
    "\n",
    "# Transform it to a DataFrame for easier plotting\n",
    "cm_df = pd.DataFrame(cm, index=train_data.classes, columns=train_data.classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot the matrix as a heatmap\n",
    "sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='d')\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.savefig(f'{exp_dir_path}/{experiment_name}_confusion_matrix.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(all_test_labels, all_test_predictions)\n",
    "\n",
    "# Transform it to a DataFrame for easier plotting\n",
    "cm_df = pd.DataFrame(cm, index=train_data.classes, columns=train_data.classes)\n",
    "\n",
    "# Calculate percentage of correctly predicted emotions\n",
    "percentage_correct = np.diagonal(cm) / np.sum(cm, axis = 1)\n",
    "# Calculate percentage of true positives\n",
    "percentage_true_positives = np.diagonal(cm) / np.sum(cm, axis = 0)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot the matrix as a heatmap\n",
    "sns.heatmap(cm_df, annot=True, cmap='Blues', fmt='d')\n",
    "\n",
    "class_names_with_percentage = [f'{class_name}\\n{percentage*100:.1f}%' for class_name, percentage in zip(train_data.classes, percentage_correct)]\n",
    "class_names_with_percentage_x = [f'{class_name}\\n{percentage*100:.1f}%' for class_name, percentage in zip(train_data.classes, percentage_true_positives)]\n",
    "\n",
    "\n",
    "plt.yticks(ticks=np.arange(len(train_data.classes)) + 0.5, labels=class_names_with_percentage, va='center')\n",
    "plt.xticks(ticks=np.arange(len(train_data.classes)) + 0.5, labels=class_names_with_percentage_x, va='center')\n",
    "\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Save the confusion matrix\n",
    "plt.savefig('confusion_matrix.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8dfacfe7",
   "metadata": {},
   "source": [
    "## Логирование модели, завершение обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01029bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем новую OutputModel\n",
    "output_model = OutputModel(\n",
    "    task=task,\n",
    "    config_dict={\n",
    "        'num_epochs': num_epochs, \n",
    "        'early_stopping': True, \n",
    "        'classes': train_data.classes,  # информация о классах\n",
    "        'batch_size': batch_size,\n",
    "        'num_workers': 4,\n",
    "        'data_augmented': data_augment\n",
    "    }\n",
    ")\n",
    "\n",
    "# Обновляем состояние модели в OutputModel\n",
    "output_model.update_weights(\n",
    "    weights_filename=model_saving_name\n",
    ")\n",
    "\n",
    "# Обозначаем модель как лучшую\n",
    "output_model.publish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f329e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()  # очищаем GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# завершаем и закрываем clearML task\n",
    "task.completed()\n",
    "task.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
